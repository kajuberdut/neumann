services:
  llama:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: llama-server
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - "${HOST_PORT}:8080"
    volumes:
      # Update the left side to your actual local model path
      - "${MODEL_DIR}:/models"
      - ./docker/entrypoint.sh:/app/entrypoint.sh
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    entrypoint: ["/bin/bash", "/app/entrypoint.sh"]
